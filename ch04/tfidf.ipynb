{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection as modsel\n",
    "import sklearn.preprocessing as preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf</h3></b>\n",
    "\n",
    "A bag-of-words representation is simple to generate but far from perfect. If we caount all words equally, then some words end up being emphasized more than we need. We'd like a document representation that emphasizes the most meaningful words, such as the main characters of of a story, words that indicate sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Tf-Idf: A Simple Twist on Bag-of-Words</h3></b>\n",
    "\n",
    "Tf-Idf is a simple twist on the bag-of-words approach. It stands for term frequency inverse document frequency. Instead of looking at raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is \n",
    "\n",
    "bow(w,d) = # times word w appears in document d\n",
    "\n",
    "tf-idf(w,d) = bow(w,d)* N / (# documents in which word w appears)\n",
    "\n",
    "N is the total number of documents. The fraction N / (# documents) is what's known as the inverse document frequency. If a word appears in many documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then is inverse document frequency is much higher.\n",
    "\n",
    "Alternatively, we can take a log transform instead of using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers smaller. If we define tf-idf as:\n",
    "\n",
    "tf-idf(w,d) = bow(w,d) * log( N / # documents in which word w appears)\n",
    "\n",
    "then a word that appears in every single document will effectively be zeroed out, and a word that appears in a very few documents will have an even larger count than before. Thus, tf-idf makes rare words more prominent and effectively ignores common words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Putting It to the Test</h4></b>\n",
    "\n",
    "Tf-idf transforms word count features through multiplication with a constant. Hence, it is an example of feature scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('../datasets/yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id',\n",
       "       'cool', 'useful', 'funny'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target signifies whether customer liked their experience - binarization of target, \n",
    "# apply to each row of series, and for apply, method that equates to axis=1 or 'columns', because we treat the rows \n",
    "# in each column as a group, find the 'stars' index and check if its value is greater than or equal to 3.\n",
    "review_df['target'] = review_df.apply(lambda x: x['stars'] >= 3, axis='columns') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = modsel.train_test_split(review_df, train_size=0.7, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 11)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 11)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Scaling Bag-of-Words with Tf-Idf Transformation</h4></b>\n",
    "\n",
    "The goal of this experiment is to compare the effectiveness of bag-of-words, tf-idf, and l<sup>2</sup> normalization for linear classification. Note that doing tf-idf then l<sup>2</sup> normalization is the same as doing l<sup>2</sup> normalization alone. So we only need to test three sets of features: bag-of-words, tf-idf, and word-wise l<sup>2</sup> normalization on top of bag-of-words.\n",
    "\n",
    "We use scikit-learn's CountVectorizer to convert the review text into a bag-of-words. All text featurization methods implicitly depend on a tokenizer, which is the module that converts a text string into a list of tokens (words). Here, scikit-learn's default tokenizing pattern looks for sequences of two or more alphanumeric characters. Punctation marks are treated as token separators.\n",
    "\n",
    "When we use training statistics to scale test data, the result will look a little fuzzy. Min_max scaling on the test on the test set no longer neatly maps to 0 and 1. l<sup>2</sup> norms, mean, and variance statistics will all look a little off. This is less problematic than missing data. For instance, the test set may contain words that are not present in the training data, and we would have no document frequencyy to use the new words. The common solution is simply drop the new words in the test set. This may seem irresponsible, but the model- trained on the training set= would not know what to do with these words anyway. A slightly less hacky option would be to explicitly learn a 'garbage' word and map all low-frequency words to it, even within the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24980"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transform = text.CountVectorizer()   # initialize transformer\n",
    "X_tr_bow = bow_transform.fit_transform(training_data['text'])  # feed only the input\n",
    "X_te_bow = bow_transform.transform(test_data['text'])\n",
    "len(bow_transform.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = training_data['target']  # store the target\n",
    "y_te = test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 24980)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf representation using the bag-of-words matrix\n",
    "\n",
    "tfidf_trfm = text.TfidfTransformer(norm=None)  # initialize transformer\n",
    "X_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow) # bag-of-words of training data go into tfidf transformer\n",
    "X_te_tfidf = tfidf_trfm.transform(X_te_bow) # avoid data leakage by using training data parameters on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2-normalzie the bag-of-words representation\n",
    "X_tr_l2 = preproc.normalize(X_tr_bow, axis=0) # normalize the features, axis=0\n",
    "X_te_l2 = preproc.normalize(X_te_bow, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subtle point about feature scaling is that it requires knowing feature statistics that we most likely do not know in practice, such as the mean, variance, document frequency, l<sup>2</sup> norm, etc. In order to compute the tf-idf representation we have to compute the inverse document frequencies based on the training data and use these statistics to scale both the training and test data. In scikit-learn, fitting the feature transformer on the training data amounts to collecting the relevant statistics. The fitted transformer can then be applied to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Classification with Logistic Regression</h4></b>\n",
    "\n",
    "Logistic regression is a simple, linear classifier. Due to its simplicity, its often a good first classifier to try. It takes a weighted combination of the input features, and passes it through a sigmoid function, which smoothly maps any real number to a number between 0 and 1. The function transforms a real number input, x, into a number between 0 and 1. It has one set of parameters, w, which represents the slope of the increase around the midpoint, 0.5. The intercept b denotes the input value where the output crosses the midpoint. A logistic classifier would predict the positive class if the sigmoid output is greater than 0.5 and the negative class otherwise. By varying w and b, one can control where that change in decision occurs, and how fast the decision should respond to changing input values around that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0, ):\n",
    "    \"\"\" Helper function to train a logistic classifier and score on that data\"\"\"\n",
    "    m = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print(f\"Test with score with {description} features: {s*100:.3f}%\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with score with bag-of-words features: 89.067%\n",
      "Test with score with l2-normalized features: 85.633%\n",
      "Test with score with tf_idf features: 88.900%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bag-of-words')\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized')\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf_idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradoxically, the results show that the most accurate classifier is the one using bag-of-words features. This was unexpected. As it turns out, the reason is that the classifiers are not well 'tuned', which is a common pitfall when comparing classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Tuning Logistic Regression</h4></b>\n",
    "\n",
    "Logisitic regression has a few bells and whistles. When the number of features is greater than the number of data points, the problem of finding the best model is said to be underdetermined. One way to fix this problem is by placing addtional constraints on the training process. This is known as regularization.\n",
    "\n",
    "In order to allow for regularization we must specify a regularization parameter. Regularization parameters are hyperparameters that are not learned automatically in the model training process. Rather, they must be tuned on the problem at hand and given to the training algorithm. This process is known as hyperparameter tuning.\n",
    "\n",
    "One basic method for tuning hyperparameters is called grid search: you specify a grid of hyperparameter values and the tuner programmaatically searches for the best hyperparameter setting in the grid. After finding the best hyperparameter settiing, you train a model on the entire training set using that setting, and use its performance on the test set as the final evaluation of this class of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Cross Validation</h4></b>\n",
    "\n",
    "We also want to test whether the difference in accuracy between tf-idf and bag-of-words is due to noise. To this end, we use k-fold cross validation to simulate having multiple statistically independent datasets. It divides the dataset into k folds. The cross validation process iterates through the folds, using all but one fold for training, and validating the results on the fold that is held out. \n",
    "\n",
    "The GridSearchCV() funcition in scikit-learn runs a grid search with cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Tune Hyperparameters When Comparing Models</h4></b>\n",
    "\n",
    "It's essential to tune hyperparameters when comparing models or features. The default settings of a software package will always return a model. But unless the software performs automatic tuning under the hood, it is likely to return a suboptimal model based on suboptimal hyperparameter settings. The sensitivity of classifier performance on hyperparameter settings depends on the model and the distribution of the training data. Logistic regression is relatively robust (or insensitive) to hyperparameter settings. Even so, it is necessary to find and use the right range of hyperparameters. Otherwise, the advantages of one model versus another may be due solel due to tuning parameters, and will not reflect the actual behavior of the model or features. Even the best autotuning packages still require specifying the upper and lower limits of search, and finding those limits can take a few manual tries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Estimating Variance via Resampling</h4></b>\n",
    "\n",
    "Modern statistical methods assume that the underlying data comes from a random distribution. The performance measurements of models derived from data are also subject to random noise. In this situation it is always a good idea to take the measurement not just once, but multiple times, based on datasets of comparable statistics. This gives us a confidence interval for the measurement. k-fold cross validation is one such strategy. Resampling is another technique that generates multiple small smaples from the same underlying dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning logisitc regression hyperparmeters with grid search\n",
    "\n",
    "# Specify a search grid, then do a 5-fold grid search for each of the feature sets\n",
    "param_grid_ = {'C':[1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "# Tune classifier for bag-of-words representation\n",
    "bow_search = modsel.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_) # instantiate grid\n",
    "bow_search.fit(X_tr_bow, y_tr) # run the k-fold cv grid search\n",
    "\n",
    "# Tune classifier for L2-normalized word vector\n",
    "l2_search = modsel.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "l2_search.fit(X_tr_l2, y_tr)\n",
    "\n",
    "# Tune classifier for tf-idf\n",
    "tfidf_search = modsel.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "tfidf_search.fit(X_tr_tfidf,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.10338268, 0.18638144, 0.38504958, 0.39609017, 0.4525372 ,\n",
       "        0.42367635]),\n",
       " 'std_fit_time': array([0.03779055, 0.02903081, 0.02645252, 0.05596991, 0.03757431,\n",
       "        0.03764445]),\n",
       " 'mean_score_time': array([0.00061378, 0.00062571, 0.00060487, 0.00063524, 0.00086622,\n",
       "        0.00069604]),\n",
       " 'std_score_time': array([2.23847495e-05, 2.61846563e-05, 5.51945962e-05, 8.68596023e-05,\n",
       "        8.25957665e-05, 9.86000320e-05]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.001, 0.1, 1.0, 10.0, 100.0],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1.0},\n",
       "  {'C': 10.0},\n",
       "  {'C': 100.0}],\n",
       " 'split0_test_score': array([0.83428571, 0.84785714, 0.89428571, 0.89714286, 0.89285714,\n",
       "        0.88857143]),\n",
       " 'split1_test_score': array([0.83428571, 0.84928571, 0.89      , 0.89071429, 0.89571429,\n",
       "        0.88428571]),\n",
       " 'split2_test_score': array([0.83428571, 0.84071429, 0.89428571, 0.88857143, 0.88714286,\n",
       "        0.88428571]),\n",
       " 'split3_test_score': array([0.83357143, 0.84285714, 0.89      , 0.88642857, 0.88071429,\n",
       "        0.87071429]),\n",
       " 'split4_test_score': array([0.83357143, 0.84428571, 0.89214286, 0.88571429, 0.88714286,\n",
       "        0.88714286]),\n",
       " 'mean_test_score': array([0.834     , 0.845     , 0.89214286, 0.88971429, 0.88871429,\n",
       "        0.883     ]),\n",
       " 'std_test_score': array([0.00034993, 0.00316228, 0.00191663, 0.00410575, 0.00519812,\n",
       "        0.00636316]),\n",
       " 'rank_test_score': array([6, 5, 1, 2, 3, 4], dtype=int32)}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out one of the grid search outputs to see how it went\n",
    "bow_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cross validation results in a box-and-whiskers plot to visualize and compare classifier performance\n",
    "search_results = pd.DataFrame.from_dict({\n",
    "    'bow'   : bow_search.cv_results_['mean_test_score'],\n",
    "    'tfidf' : tfidf_search.cv_results_['mean_test_score'],\n",
    "    'l2'    : l2_search.cv_results_['mean_test_score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "ax = sns.boxplot(data=search_results, width=0.4)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.tick_params(labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>l2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.877857</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.892143</td>\n",
       "      <td>0.891571</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.889714</td>\n",
       "      <td>0.886857</td>\n",
       "      <td>0.843143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.888714</td>\n",
       "      <td>0.875571</td>\n",
       "      <td>0.864429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.869714</td>\n",
       "      <td>0.876857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bow     tfidf        l2\n",
       "0  0.834000  0.834286  0.834000\n",
       "1  0.845000  0.877857  0.834000\n",
       "2  0.892143  0.891571  0.834000\n",
       "3  0.889714  0.886857  0.843143\n",
       "4  0.888714  0.875571  0.864429\n",
       "5  0.883000  0.869714  0.876857"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results.index = pd.Index(param_grid_['C'])\n",
    "search_results.index.name = 'regularization parameter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>l2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regularization parameter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00100</th>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.877857</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10000</th>\n",
       "      <td>0.892143</td>\n",
       "      <td>0.891571</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00000</th>\n",
       "      <td>0.889714</td>\n",
       "      <td>0.886857</td>\n",
       "      <td>0.843143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.00000</th>\n",
       "      <td>0.888714</td>\n",
       "      <td>0.875571</td>\n",
       "      <td>0.864429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.00000</th>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.869714</td>\n",
       "      <td>0.876857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               bow     tfidf        l2\n",
       "regularization parameter                              \n",
       "0.00001                   0.834000  0.834286  0.834000\n",
       "0.00100                   0.845000  0.877857  0.834000\n",
       "0.10000                   0.892143  0.891571  0.834000\n",
       "1.00000                   0.889714  0.886857  0.843143\n",
       "10.00000                  0.888714  0.875571  0.864429\n",
       "100.00000                 0.883000  0.869714  0.876857"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with score with bow features: 89.033%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with score with l2_normalized features: 88.167%\n",
      "Test with score with tf-idf features: 89.633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/godfredowusu/Desktop/mlbooklearn/tfenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train a final model on the entire training set, using the best hyperparameter settings. Measure test set accuracy \n",
    "\n",
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow', _C=bow_search.best_params_['C'])\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2_normalized', _C=l2_search.best_params_['C'])\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf', _C=tfidf_search.best_params_['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper tuning improved the accuracy of all the feature sets, and all three now yield similar classification accuracy under regularized logistic regression. The accuracy score of the tf-idf model is slightly higher, but the difference is likely not statistically significant. These results are completely mystifying. If feature scaling doesnt work better than vanilla bag-of-words, then why do it at all? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Deep Dive: What Is Happening?</h4></b>\n",
    "\n",
    "In order to understand the 'why' behind the results, we have to look at how the features are being used by the model. For linear models like logistic regression, this happens through an intermediary object called the data matrix. The dat matrix contains data points represented by fixed-length flat vectors. \n",
    "\n",
    "With bag-of-words vectors, the data matrix is also known as the document-term metrix. To form a document-term matrix, simply take the document vectors, lay them out flat, and stack them on top of one another. The columns represent all possible words in the vocabulary. Since most documents contain only a small subset of all possible words, most of the entries in this matrix are zeros; it is a sparse matrix. Feature scaling methods are essentially column operations on the data matrix. In particular, tf-idf and l<sup>2</sup> normalization both multiply the entire column (an n-gram feature) by a constant, ie column scaling.\n",
    "\n",
    "Training a linear classifier boils down to finding the best linear combination of features, which are column vectors of the data matrix. The solution space is characterized by the column space and the null space of the data matrix. The quality of the trained linear classifier directly depends upon the null space and the column space of the data matrix. A large column space means that there is little linear dependency between the features, which is generally good. Te null space contains 'novel' data points that cannot be formulated as linear combinations of existing dta; a large null space could be problematic.\n",
    "\n",
    "How do column scaling operations affect the column space and the null space of the data matrix? The answer is 'Not very much'. But there is a small chance that tf-idf and l<sup>2</sup> normalization could be different.\n",
    "\n",
    "The null space of the data matrix can be large for a couple of reasons. First, many datasets contain data points that are very similar to one another. This means the effective row space is small compared to the number of data points in the dataset. Second, the number of features can be much larger than the number of data points. Bag-of-words is particularly good at creating giant feature spaces. In the example used, there are 24980 features in 7000 reviews in the training set. Moreover, the distinct words usually grows with the number of documents in the dataset, so adding more documents would not necessarily decrease the feature-to-data ratio or reduce the null space.\n",
    "\n",
    "With bag-of-words, the column space is relatively small compared to the number of features. There could be words that appear roughly the same number of times in the same documents. This would lead to a corresponding column vector being nearly linearly dependent, which leads to the column space being not as full rank as it could be. This is called a rank deficiency.\n",
    "\n",
    "Rank-deficient row space and column space lead to the model being overly provisioned for the problem. The linear model outfits a weight parameter for each feature in the dataset. If the row and column space were full rank, then the model would allow us to generate any target vector in the output space. When they are rank deficient, the model has more degrees of freedom than it needs. This makes it harder to pin down a solution.\n",
    "\n",
    "Can feature scaling solve the rank deficiency problem of the data matrix? The column space is defined as the linear combination of all column vectors: a<sub>1</sub>v<sub>1</sub> + a<sub>2</sub>v<sub>2</sub> + ... + a<sub>n</sub>v<sub>n</sub>. Feature scaling replaces a column vector with a constant multiple, say v<sub>1</sub> = cv<sub>1</sub>. But we can still generate the original linear combination by just replacing a<sub>1</sub> with a~<sub>1</sub> = a<sub>1</sub>/c. It appears that feature scaling does not change the rank of the column space. Similarly, feature scaling does not affect the rank of the null space, because one can counteract the scaled feature column by reverse scaling the corresponding entry in the weight vector.\n",
    "\n",
    "However, there is one catch. If the scalar is 0, then there is no way to recover the original linear combination; v<sub>1</sub> is gone. If that vector is linearly independent from all the other columns, then we've effectively shrunk the column space and enlarged the null space.\n",
    "\n",
    "If that vector is not correlated with the target output, then this is effectively pruning away noisy signals, which is a good thing. This turns out to be the key difference between tf-idf and l<sup>2</sup> normalization. l<sup>2</sup> normalization would never compute a norm of zero, unless the vector contains all zeros. If the vector is close to zero, then its norm is also close to zero. Dividing the small norm would accentuate the vector and make it longer.\n",
    "\n",
    "Tf-idf, on the other hand, can generate scaling factors that are close to zero. This happens when the word is in a large number of documents in the training set. Such a word is likely not strongly correlated with the target vector. Pruning it away allows the solver to focus on the other directions in the column space and find better solutions.\n",
    "\n",
    "Where the feature scaling - both l<sup>2</sup> and tf-idf - does have a telling effect is on the convergence speed of the solver. This is a sign that the data matrix now has a much smaller condition number (the ratio between the largest and smallest singular values). In fact, l<sup>2</sup> normalization makes the condition number nearly 1, making the model much faster to train. But its not the case that the better the condition number, the better the solution. During this experiment, l<sup>2</sup> normalization converged much faster than either BoW or tf-idf. But it is also more sensitive to overfitting: it requirs much more regularization and is more sensitive to the number of iterations during optimization.\n",
    "\n",
    "The lesson is: the right feature scaling can be helpful for classification. The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix, making the model easier to train. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
