{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Categorical Variables: Counting Eggs in the Age of Robotic Chickens</h3></b>\n",
    "\n",
    "A categorical variable, as the name suggests, is used to represent categories or labels. For instance, a categorical variable could represent major cities in the world, the four seasons in a year, the industry of a company etc. The number of category values is always finite in a real-worlld dataset. The values may be represented numerically. However, unlike other numerical variables, the values of a categorical variable cannot be ordered with respect to one another. They are called nonordinal. \n",
    "\n",
    "The vocabulary of a document corpus can be interpreted as a large categorical variable, with the categories being unique words.  It can be computationally expensive to represent so many distinct categories. If a category appears multiple times in a data point (document), then we can represent it as a count, and represent all of the categories through their count statistics. This is called bin counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Encoding Categorical Variables</h3></b>\n",
    "\n",
    "The categories of a categorical variable are usually not numeric. Eye color can be 'black', 'blue', 'brown' etc. Thus an encoding method is needed to turn these nonnumeric categories into numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>One-Hot Encoding</h4></b>\n",
    "\n",
    "A better method is to use a group of bits. Each bit represent a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be 'on'. This is called one-hot encoding, and it is implemented i scikit-learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus a categorical variable with k possible categories is encoded as a feature vector of length k.\n",
    "\n",
    "One hot encoding uses one more bit than is strictly necessary. If we see that k-1 of the bits are 0, then the last bit must be 1 because the variable must take on one of the k values .( Mathematically, one can write this constraint as \"the sum of all bits must be equal to 1\".) Thus, we have a linear dependency on our hands, Linear dependent features mean that the trained linear models will not be unique. Different linear combinations of the features can make the same predictions so it would be difficult explaining the effect of a feature on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Dummy Coding</h4></b>\n",
    "\n",
    "The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k-1. Dummy coding removes the extra degree of freedom by using only k-1 features in the representation. One feature is thrown under the buss and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in pandas as pandas.get_dummies (to get dummy coding specify drop_first=True). The outcome of modeling with dummy coding is more interpretable than with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Rent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SF</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SF</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SF</td>\n",
       "      <td>4001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYC</td>\n",
       "      <td>3499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYC</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NYC</td>\n",
       "      <td>3501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      City  Rent\n",
       "0       SF  3999\n",
       "1       SF  4000\n",
       "2       SF  4001\n",
       "3      NYC  3499\n",
       "4      NYC  3500\n",
       "5      NYC  3501\n",
       "6  Seattle  2499\n",
       "7  Seattle  2500\n",
       "8  Seattle  2501"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a toy dataset of apartment rental prices in New York, San Francisco, and Seattle\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'City' : ['SF', 'SF', 'SF', 'NYC', 'NYC', 'NYC', 'Seattle', 'Seattle', 'Seattle'],\n",
    "    'Rent' : [3999,4000,4001,3499,3500,3501,2499,2500,2501]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333.3333333333335"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Rent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical variables in the DataFrame to one-hot encoding and fit a linear regression model\n",
    "one_hot_df = pd.get_dummies(df, prefix=['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rent</th>\n",
       "      <th>city_NYC</th>\n",
       "      <th>city_SF</th>\n",
       "      <th>city_Seattle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3999</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4001</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3499</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3501</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2499</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2501</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rent  city_NYC  city_SF  city_Seattle\n",
       "0  3999     False     True         False\n",
       "1  4000     False     True         False\n",
       "2  4001     False     True         False\n",
       "3  3499      True    False         False\n",
       "4  3500      True    False         False\n",
       "5  3501      True    False         False\n",
       "6  2499     False    False          True\n",
       "7  2500     False    False          True\n",
       "8  2501     False    False          True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(one_hot_df[['city_NYC', 'city_SF', 'city_Seattle']], one_hot_df['Rent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 166.66666667,  666.66666667, -833.33333333])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333.3333333333335"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rent</th>\n",
       "      <th>city_SF</th>\n",
       "      <th>city_Seattle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3999</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3499</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3501</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2499</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2501</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rent  city_SF  city_Seattle\n",
       "0  3999     True         False\n",
       "1  4000     True         False\n",
       "2  4001     True         False\n",
       "3  3499    False         False\n",
       "4  3500    False         False\n",
       "5  3501    False         False\n",
       "6  2499    False          True\n",
       "7  2500    False          True\n",
       "8  2501    False          True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a linear regression model on dummy code\n",
    "# Specify the 'drop_first' flag to get dummy coding\n",
    "dummy_df = pd.get_dummies(df, prefix=['city'], drop_first=True)\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  500., -1000.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dummy_df[['city_SF', 'city_Seattle']], dummy_df['Rent'])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one-hot encoding, the intercept term represents the global mean of the target variable. Rent and each of the linear coefficients represent how much that city's average rent differs from the global mean. \n",
    "\n",
    "With the dummy coding, the bias coefficient represents the mean value of the response variable y for the reference category. The coefficient for the ith feature is equal to the difference between the mean response value for the ith category and the mean of the reference category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Effect Coding</h4></b>\n",
    "\n",
    "Another variant of categorical variable encoding is effect coding. Effect codiing is very similar to dummy coding with the difference that the reference category is now represented by the vector of all -1's. Effect coding is very similar to dummy coding, but results in linear regression models that are even simpler to interpret. The intercept term represents the global mean of the target variable, the individual coefficients indicate how much the means of the individual categories differ from the global mean. (This called the main effect of the category or level, hence the name 'effect coding'). \n",
    "\n",
    "Comparing the model built by effect coding to previous results, you realize that one-hot encoding actually come up with\n",
    "the same intercept and coefficients, but in that case there are linear coefficients for each city. In effect coding, no single feature represents the reference category, so the effect of the reference category needs to be separately computed as the negative sum of the coefficients of all other categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rent</th>\n",
       "      <th>city_SF</th>\n",
       "      <th>city_Seattle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3999</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3499</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2499</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2501</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rent city_SF city_Seattle\n",
       "0  3999    True        False\n",
       "1  4000    True        False\n",
       "2  4001    True        False\n",
       "3  3499    -1.0         -1.0\n",
       "4  3500    -1.0         -1.0\n",
       "5  3501    -1.0         -1.0\n",
       "6  2499   False         True\n",
       "7  2500   False         True\n",
       "8  2501   False         True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_df = dummy_df.copy()\n",
    "effect_df.loc[3:5, ['city_SF', 'city_Seattle']] = -1.0\n",
    "effect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(effect_df[['city_SF', 'city_Seattle']], effect_df['Rent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 666.66666667, -833.33333333])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333.3333333333335"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Pros and Cons of Categorical Variable Encodings</h4></b>\n",
    "\n",
    "One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiple valid models for the same problem. The nonuniqueness is sometimes problematic for interpretation, but the advantage is that each feature corresponds to a category. Moreover, missing data can be encoded as the all-zero vector, and the output should be the overall mean of the target variable.\n",
    "\n",
    "Dummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange.\n",
    "\n",
    "Effect coding avoids this problem by using a different code for the reference category, but the vector of all -1's is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding.\n",
    "\n",
    "All three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Dealing with Large Categorical Variables</h3></b>\n",
    "\n",
    "Automated data collection on the internet can generate large categorical variables. This is common in applications such as targeted advertising and fraud detection. \n",
    "\n",
    "In targeted advertising, the task is to match a user with a set of ads. Features include the user ID, the website domain for the ad, the search query, the current page, and all possible pairwise conjunctions of those features. Each of these is a very large categorical variable. The challenge is to find a good feature representation that is memory efficient, yet produces accurate models that are fast to train. Existing solutions are as follows:\n",
    "- Do nothing fancing with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model.\n",
    "- Compress the features. There are two choices;\n",
    "a. Feature hashing, popular with linear models\n",
    "b. Bin counting, popular with linear models as well as trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Feature Hashing</h4></b>\n",
    "\n",
    "A hash function is a deterministic function that maps a potentially unbounded integer to a finite integer range [1,m]. Since the input domain is potentially larger than the output range, multiple numbers may get mapped to the same output. This is called a collision. A uniform hash function ensures that roughly the same number of numbers are mapped into each of the m bins.\n",
    "\n",
    "We can think of a hash function as a machine that intakes numbered balls (keys) and routes them to one of m bins. Balls with the same number will always get routed to the same bin. This maintains the feature space while reducing the storage and processing time during machine learning training and evaluation cycles.\n",
    "\n",
    "Hash functions can be constructed for any object that can be represented numerically (which is true for any data that can be stored on a computer).\n",
    "\n",
    "When there are many features, storing the feature vector could take up a lot of space. Feature hashing compresses the original feature vector into an m-dimensional vector by applying a hash function to the feature ID. For instance, if the original features were words in a document, then the hashed version would have a fixed vocubulary size of m, no matter how many uniqu words were in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature hashing for word features\n",
    "def hash_features(word_list, m):\n",
    "    output = [0]*m\n",
    "    for word in word_list:\n",
    "        index = hash(word) % m\n",
    "        output[index] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variation of feature hashing adds a sign component, so that counts are either added to or subtracted from the hashed bin. Statistically speaking, this ensures that the inner products beween hashed features are equal in expectation to those of the original features. The value of the inner product after hashing is within O(1/√m) of the original inner product, so the size of the hash table m can be selected based on acceptable errors. In practice, picking the right m could take some trial and error.\n",
    "\n",
    "Feature hashing can be used for models that involve the inner product of feature vectors and coefficients, such as linear models and kernel methods. It has been demonstrated to be successful in the task of spam filtering. One downside of feature hashing is that the hashed features, being aggregates of original features, are no longer interpretable.\n",
    "\n",
    "Feature hashing benefits us computationally, sacrificing immediate user interpretability. This is an easy trade-off to accept when progressing from data exploration and visualization into a machine learning pipeline for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signed feature hashing\n",
    "def hash_features(word_list, m):\n",
    "    output = [0]*m\n",
    "    for word in word_list:\n",
    "        index = hash(word) % m\n",
    "        sign_bit = hash(word) % 2\n",
    "        if (sign_bit == 0):\n",
    "            output[index] -= 1\n",
    "        else:\n",
    "            output[index] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature hashing (aka the hashing trick)\n",
    "review_df = pd.read_csv('../datasets/yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4174"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(review_df['business_id'].unique())  # 4174 unique values in business_id categorical column\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "output = []\n",
    "for value in review_df.business_id.values:\n",
    "    output.append([value])\n",
    "\n",
    "h = FeatureHasher(n_features=m, input_type='string')\n",
    "\n",
    "f = h.transform(output) # expects a list of a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9yKzy9PApeiPPOUJEtnvkg',\n",
       " 'ZRJwVLyzEJq1VAihDhYiow',\n",
       " '6oRAC4uyJCsJl1X0WZpVSA',\n",
       " '_1QQZuf4zZOyFCvXc0o6Vg',\n",
       " '6ozycU1RpktNG2-1BroVtw']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['business_id'].unique().tolist()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our pandas Series, in bytes 790144\n",
      "Our hashed numpy array, in bytes 48\n",
      "Our one-hot encoded numpy array, in bytes 41740144\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "print('Our pandas Series, in bytes', getsizeof(review_df['business_id']))\n",
    "print('Our hashed numpy array, in bytes', getsizeof(f))\n",
    "print('Our one-hot encoded numpy array, in bytes', getsizeof(\n",
    "                pd.get_dummies(review_df['business_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4174)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Bin Counting</h4></b>\n",
    "\n",
    "Bin counting is one of the perennial rediscoveries in machine learning. It has been reinvented and used in a variety of applications, from ad clicking through rate prediction to hardware branch prediction. The idea of bin counting is simple : rather than using the value of the categorical value as the feature, instead use use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. Under Bayes classifiers, this statistic is the conditional probability of the class under the assumption that all features are independent.\n",
    "\n",
    "Bin counting assumes that historical data is available for computing the statistics. For example say we have click data for users, then because on the number of times a user has clicked on any add and the number of times the user has not clicked, we can compute the probability of the user clicking on any ad P(click) = # clicks/ (# clicks + # non-clicks).  Similarly, for any Queryhash - AdDomain combination we can compute the probability of a click  ie P(click) = # clicks/ (# clicks + # non-clicks). At training time, every time we see a specific user, we can use their probability  of click as the input feature to the model. The same goes for the QueryHash - AdDomain pairs.\n",
    "\n",
    "Suppose there were 10,000 users. One-hot encoding would generate a sparse vector of length 10,000, with a single 1 in the column that corresponds to the value of the current data point. Bin counting would encode all 10,000 binary columns with a single feature with a real value between 0 and 1. In a weird way we moved from categorical feature to real-valued feature but in this case it can actually make sense since users with 0.9 chance of clicking are of a bigger interest than users with 0.1 chance of clicking. So this is better than leaving it to the model to realize which users are more likely to click, yay, feature engineering.\n",
    "\n",
    "We can include other features in addition to the historical click-through probability: the raw counts themselves (number of clicks and nonclicks), the log-odds ratio, or any other derivatives of probabilityy. Our example used here is for predicting ad click-through rates, but the technique readily applies to general binary classification. It can also be readily extended to multiclass classification usuing the usual techniques to extend binary classifiers to multiclass; ie via one-against-many odds ratios or other multiclass label encodings.\n",
    "\n",
    "In short, bin counting converts a categorical variable into statistics about the value. It turns a large, sparse, binary representation of the categorical variable, such as that produced by one-hot encoding, into a very small, dense, real-valued numeric representation. (The rest of the statistics can be derived on the fly from the raw counts.) Hence it requires O(k) space, where k is the number of unique values of the categorical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Odds Ratio and Logs Odds Ratio for Bin Counting</h4></b>\n",
    "\n",
    "The odds ratio is usually defined between two binary variables. It looks at their strength of association by asking the question, \"How much more likely is it for Y to be true when X is true? For instance, we might ask, \"How much more likely is a user to click on an ad than the general population?\" Here X is the binary variable for the \"current user  versus the rest of the population\" and Y is the variable \"click on add or not\". The computation uses what's called the two-way contingency table (basically, four numbers that correspond to the four possible combinations of X and Y).\n",
    "\n",
    "Given an input variable X and a target variable Y, the odds ratio is defined as:\n",
    "\n",
    "$odds\\;ratio$ = $\\frac{ P(Y = 1 | X = 1) / P(Y = 0 | X = 1) }{ P(Y = 1 | X = 0)/ P( Y = 0 | X = 0)}$\n",
    "\n",
    "More simpy, we can just look at the numerator, which examines how much more likely that a single user clicks on an ad versus not clicking. This is suitable for large categorical variables with many values, not just two: \n",
    "\n",
    "$odds\\;ratio (user, ad click)$ = $\\frac{P(Y = click | X = user)}{P(Y = not click | X = user)}$ \n",
    "\n",
    "In our example, this translates as the ratio between \"how much more likely is it that a specific user clicks on an ad rather than does not click\" and \"how much more likely is it that other people click rather than not click\".\n",
    "\n",
    "Probability ratios can easily become very small or very large. For instance, there will be users who almost never click on ads, and perhaps users who click on ads much more frequently than not. The log transform again comes to our rescue. Another useful property of the logarithm is that it turns a division into a subtraction.\n",
    "\n",
    "$log$-$odds$ $ratio (user, ad click)$  = $logP(Y = click | X = user) - log P(Y = not click | X = user)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h4>Bin-counting example</h4></b>\n",
    "\n",
    "To illustrate bin counting in practice, we'll use data from a Kaggle competition hosted by Avazu. Here are\n",
    "some relevant statistics about the dataset:\n",
    "\n",
    "- There are 24 variables, including click, a binary click/no click counter, and device_id, which tracks which device\n",
    "an ad was displayed on.\n",
    "\n",
    "The aim of the Avazu competition was to predict click-through rate using ad data, but we will use the dataset to \n",
    "demonstrate how bin counting can greatly reduce the feature space for large amounts of streaming data.\n",
    "\n",
    "The dataset here has 50,000 records that have a total of 8466 devices. For each device, we want to calculate the generate the bin counting statistics of number of clicks, number of no clicks, total number of ads seen, probability of clicks, probability of no clicks and the odds ratio.\n",
    "\n",
    "To generate the number of clicks for each device id, we index all rows where the value in the clicks column, is greater than 0, then select the device id column and use the value_counts() function. The value_counts() function which will give us how many times each device clicked an ad, as a Series, with device_id as the index and the count as the value.\n",
    "We create a new Series out of this, so that we get to name the Series this time as the click series. \n",
    "\n",
    "Next, we select all the rows where the click column is less than 1 ie 0, select the device id column and again use the value_counts() function to generate the number of times each device did not click an add, converting the result to a a Series, so that we get the chance to name each Series, called no_click. \n",
    "\n",
    "Then with the two Series, we create a Dataframe, the Series names become column names in the DataFrame, the device_id will be the index and we fill all NA values with 0. Finally we count the total number of times each device saw an ad by summing the clicks and no clicks columns, but before we do that we first convert each column to an int64 dtype. Now we have generated our statistics.\n",
    "\n",
    "To bin the counts, we calculate the probability of clicks by dividing each count value by the total ads seen values, storing the result in its own column. We also calculate the no click probability by dividing the each no click value by its corresponding total ads seen value. Due to pandas vectorized implementation, we get the probabilities by dividing the click or no click column by the total ads seen column respectively but we first convert each column into dtype int64. Next, we calculate the odds ratio of clicking by dividing the click probability to the no click probability. We could use np.log10() to convert this to logarithm scale to further bin the real-valued odds-ratio into logarithm bins.\n",
    "We now what we need to replace each device id, with statistics and probabilities that describe its activity.\n",
    "\n",
    "In the implementation we use the filter() function. This function can take column / row labels as the items keyword, or it can be used to filter based on regex or even like keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/50krecords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 24)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8466"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique features for device_id categorical variable\n",
    "len(df['device_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each category, we want to calculate:\n",
    "# Theta = [counts, p(click), p(no click), p(click)/p(no click)]\n",
    "\n",
    "def click_counting(x, bin_column):\n",
    "    clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(),\n",
    "                       name='clicks') # clicked ads rows, for device_id get how many times user clicked\n",
    "    no_clicks = pd.Series(x[x['click']< 1][bin_column].value_counts(),\n",
    "                          name='no_clicks') # unclicked ads rows, for device_id, get how many times user didnt click\n",
    "    counts = pd.DataFrame([clicks, no_clicks]).T.fillna('0') # device_id index, # times clicked, # times not clicked\n",
    "\n",
    "    counts['total_clicks'] = counts['clicks'].astype('int64') + counts['no_clicks'].astype('int64') # total ads col\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_counting(counts):\n",
    "    counts['N+'] = counts['clicks'].astype('int64').divide(counts['total_clicks'].astype('int64')) # prob click\n",
    "    counts['N-'] = counts['no_clicks'].astype('int64').divide(counts['total_clicks'].astype('int64')) # prob no_click\n",
    "    counts['log_N+'] = counts['N+'].divide(counts['N-']) # odds ratio\n",
    "    bin_counts = counts.filter(items=['N+', 'N-', 'log_N+']) # items=col labels ,filter also takes regex= or like=\n",
    "    return counts, bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin counts example: device_id\n",
    "bin_column = 'device_id'\n",
    "device_clicks = click_counting(df.filter(items=[bin_column, 'click']), bin_column)\n",
    "device_all, device_bin_counts = bin_counting(device_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8466"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure we have all the devices\n",
    "len(device_bin_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicks</th>\n",
       "      <th>no_clicks</th>\n",
       "      <th>total_clicks</th>\n",
       "      <th>N+</th>\n",
       "      <th>N-</th>\n",
       "      <th>log_N+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a99f214a</th>\n",
       "      <td>7143.0</td>\n",
       "      <td>34184.0</td>\n",
       "      <td>41327</td>\n",
       "      <td>0.172841</td>\n",
       "      <td>0.827159</td>\n",
       "      <td>0.208957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0f7c61dc</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c357dbff</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936e92fb</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           clicks no_clicks  total_clicks        N+        N-    log_N+\n",
       "device_id                                                              \n",
       "a99f214a   7143.0   34184.0         41327  0.172841  0.827159  0.208957\n",
       "0f7c61dc     17.0       7.0            24  0.708333  0.291667  2.428571\n",
       "c357dbff     15.0       7.0            22  0.681818  0.318182  2.142857\n",
       "936e92fb      1.0      12.0            13  0.076923  0.923077  0.083333"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_all.sort_values(by='total_clicks', ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N+</th>\n",
       "      <th>N-</th>\n",
       "      <th>log_N+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a99f214a</th>\n",
       "      <td>0.172841</td>\n",
       "      <td>0.827159</td>\n",
       "      <td>0.208957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0f7c61dc</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>2.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c357dbff</th>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3cdb4052</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afeffc18</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303b711</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1e3adcb</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1f201274</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95dd94cb</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75af56c0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8466 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 N+        N-    log_N+\n",
       "device_id                              \n",
       "a99f214a   0.172841  0.827159  0.208957\n",
       "0f7c61dc   0.708333  0.291667  2.428571\n",
       "c357dbff   0.681818  0.318182  2.142857\n",
       "3cdb4052   0.750000  0.250000  3.000000\n",
       "afeffc18   0.250000  0.750000  0.333333\n",
       "...             ...       ...       ...\n",
       "5303b711   0.000000  1.000000  0.000000\n",
       "f1e3adcb   0.000000  1.000000  0.000000\n",
       "1f201274   0.000000  1.000000  0.000000\n",
       "95dd94cb   0.000000  1.000000  0.000000\n",
       "75af56c0   0.000000  1.000000  0.000000\n",
       "\n",
       "[8466 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.click.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h5>What about rare categories</h5></b>\n",
    "\n",
    "Just like rare words, reare categories require special treatment. Think about a user who logs in once a year: there will be very little data to reliably estimate that user's click-through rate for ads. Moreover, rare categories waste sapce in the counts table.\n",
    "\n",
    "One way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin. If the (total category) count is greater than a certain threshold, the the category get its own statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a single rare category to the statistics computed on all rare categories. When using the back-off method, it helsps to also add a binary indicator for whether or not the statistics come from the back-off bin.\n",
    "\n",
    "There is another way to deal with this problem, called the count-min sketch. In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table can be made smaller than k, the number of categories, and still retain low overall collision probability. This way, instead of dropping the rare categories we use hashing to reduce the storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h5>Guarding against data leakage</h5></b>\n",
    "\n",
    "Since bin counting relies on historical data to generate the necessary statistics, it requires waiting through a data collection period, incurring a slight delay in the learning pipeline. Also, when the data distribution changes, the counts need to be updated. The faster the data changes, the more frequently the counts need to be recomputed. This is particularly important for applications like targeted advertising, where user preferences and popular queries change very quickly, and lack of adaptation to the current distribution could mean huge losses for the advertising platform.\n",
    "\n",
    "One might ask, why not use the same dataset to compute the relevant statistics and train the model? The big problem here is that the statistics involve the target variable, which is what the model tries to predict. Using the output to compute the input features leads to a pernicious problem known as leakage. In short, leakage means that information is revealed that gives it an unrealistic advantage to make better predictions. This could happen when test data is leaked into the training set, or when data from the future is leaked to the past. Any time that a model is given information that it shouldn't have access to when it is making predictions in real time in production, there is leakage.\n",
    "\n",
    "If the bin-counting procedure used the current data point's label to compute part of the input statistic, that would constitute direct leakage. One way to prevent that is by instituting strict separation between count collection (for computing bin-count statistics) and training. We use an earlier batch of data points for counting, use current data points for training (mapping categorical variables to historical statistics we just collected) and use future data points for testing. That is we use time windows to prevent leakage during bin counting. This fixes the problem of leakage, but introduces the aforementioned delay (the input statistics and therefore the model will trail behind current data).\n",
    "\n",
    "It turns out that there is another solution, based on differential privacy. A statistic is approximately leakage-proof if tis distribution stays roughly the same with or without any one data point. In practice, adding a small random noise with distribution Laplace(0,1) is sufficient to cover up any potential leakage from a single data point. This idea can be combined with leaving-one-out counting to formulate statistics on current data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h5>Counts without bounds</h5></b>\n",
    "\n",
    "If the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model 'knows' the input data up to the observed scale. A trained decision tree might say, 'When x is greater than 3, predict 1'. A trained linear model might say, 'Multiply x by 0.7 and see if the result is greater than the global average'. These might be the correct decisions when x lies between 0 and 5. But what happens beyond that? No one knows.\n",
    "\n",
    "When the input counts increase, the model will need to be retrained to adapt to the current scale. If the counts accumulate rather slowly, then the effective scale won't change too fast, and the model will not need to be retrained too frequently. But when the counts increment very quickly, frequent training will be a nuisance.\n",
    "\n",
    "For this reason, it is often better to use normalized counts that are guaranteed to be bounded in a known interval. For instance, the estimated click-through probability is bounded between [0,1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n",
    "\n",
    "Neither method will guard against shifting input distributions, eg seasonal style changes etc. The model will need to be retrained to accomodate these more fundamental changes in input data distribution, or the whole pipeline will need to move on to an online learning setting where the model is continuously adpating to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
